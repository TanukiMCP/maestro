# Copyright (c) 2025 TanukiMCP Orchestra
# Licensed under Non-Commercial License - Commercial use requires approval from TanukiMCP
# Contact tanukimcp@gmail.com for commercial licensing inquiries

"""
MAESTRO Execution Engine

This engine takes a generated Workflow plan and executes it task by task.
It uses an LLM to simulate agentic behavior for tool selection and validation,
bringing the orchestrated plan to life.
"""

import asyncio
import logging
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
import json

from .schemas import Workflow, Task
from .llm_client import LLMClient, LLMResponse

logger = logging.getLogger(__name__)

class TaskStatus:
    """Enum-like class for task execution status."""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILURE = "failure"
    VALIDATION_FAILED = "validation_failed"

class TaskResult:
    """Stores the outcome of a single executed task."""
    def __init__(self, task: Task, status: str, output: Any = None, error: Optional[str] = None):
        self.task_id = task.task_id
        self.description = task.description
        self.status = status
        self.output = output
        self.error = error

class WorkflowResult:
    """Stores the final results of an entire workflow execution."""
    def __init__(self, workflow: Workflow, status: str, task_results: List[TaskResult]):
        self.workflow_id = workflow.workflow_id
        self.overall_goal = workflow.overall_goal
        self.final_status = status
        self.task_results = task_results

class ExecutionEngine:
    """
    Executes a MAESTRO workflow, simulating agentic decision-making for each task.
    """
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        # This would be a real tool dispatcher in a production system
        self.tool_registry = {
            "maestro_search": self.maestro_search_mock,
            "maestro_iae": self.maestro_iae_mock,
            "maestro_execute": self.maestro_execute_mock,
            "maestro_error_handler": self.maestro_error_handler_mock,
        }

    async def execute_workflow(self, workflow: Workflow) -> WorkflowResult:
        """
        Executes a workflow plan sequentially, task by task.
        
        Args:
            workflow: The Workflow object generated by the OrchestrationEngine.
            
        Returns:
            A WorkflowResult containing the outcome of the execution.
        """
        logger.info(f"🚀 Starting execution for workflow: {workflow.workflow_id}")
        task_results: List[TaskResult] = []
        overall_status = TaskStatus.SUCCESS

        for task in workflow.tasks:
            logger.info(f"🔄 Executing Task {task.task_id}: {task.description}")
            task_result = await self._execute_task(task, task_results)
            task_results.append(task_result)

            if task_result.status != TaskStatus.SUCCESS:
                logger.error(f"❌ Task {task.task_id} failed. Halting workflow execution.")
                overall_status = TaskStatus.FAILURE
                break # Stop execution on first failure

        logger.info(f"✅ Workflow execution finished for {workflow.workflow_id} with status: {overall_status}")
        return WorkflowResult(workflow=workflow, status=overall_status, task_results=task_results)

    async def _execute_task(self, task: Task, previous_results: List[TaskResult]) -> TaskResult:
        """Executes a single task using a simulated agentic process."""
        try:
            # 1. Agentic Tool Selection
            tool_name, tool_args = await self._agent_select_tool(task, previous_results)

            # 2. Tool Execution
            logger.info(f"  🛠️ Agent selected tool: '{tool_name}' with args: {tool_args}")
            if tool_name not in self.tool_registry:
                raise ValueError(f"Tool '{tool_name}' is not registered in the execution engine.")
            
            tool_function = self.tool_registry[tool_name]
            tool_output = await tool_function(**tool_args)
            logger.info(f"  📄 Tool '{tool_name}' produced output: {str(tool_output)[:100]}...")

            # 3. Agentic Validation
            is_valid = await self._agent_validate_output(task, tool_output)
            if not is_valid:
                logger.warning(f"  ⚠️ Validation failed for Task {task.task_id}.")
                return TaskResult(task, TaskStatus.VALIDATION_FAILED, output=tool_output, error="Output did not meet validation criteria.")

            logger.info(f"  👍 Validation successful for Task {task.task_id}.")
            return TaskResult(task, TaskStatus.SUCCESS, output=tool_output)
            
        except Exception as e:
            logger.error(f"  ❌ An error occurred during task execution: {e}", exc_info=True)
            return TaskResult(task, TaskStatus.FAILURE, error=str(e))

    async def _agent_select_tool(self, task: Task, previous_results: List[TaskResult]) -> tuple[str, dict]:
        """Simulates an LLM agent selecting which tool to use for a task."""
        prompt = f"""
You are the '{task.agent_profile.name}'.
Your system prompt is: "{task.agent_profile.system_prompt}"
Your current task is: "{task.description}"

Based on your task, you must choose exactly one tool to use from the following list:
{json.dumps(task.tools, indent=2)}

History of previous task results:
{json.dumps([res.__dict__ for res in previous_results], indent=2, default=str)}

Respond with a JSON object containing the "tool_name" you chose and a "tool_args" object with the arguments to pass to it.
For example: {{"tool_name": "maestro_search", "tool_args": {{"query": "your search query"}}}}
"""
        # In a real system, you'd parse the LLM response. Here we mock it.
        # This mock will just pick the first tool and create a plausible argument.
        selected_tool = task.tools[0] if task.tools else ""
        mock_args = {"query": task.description} if "search" in selected_tool else {"input": task.description}
        logger.info(f"  🤖 (Simulated) Agent Prompt for tool selection:\n{prompt}")
        return selected_tool, mock_args

    async def _agent_validate_output(self, task: Task, output: Any) -> bool:
        """Simulates an LLM agent validating the output of a tool."""
        prompt = f"""
You are a meticulous 'Validator-Agent'.
Your task is to determine if the output of a tool meets the required validation criteria.

Tool Output:
```
{json.dumps(output, indent=2, default=str)}
```

Validation Criteria:
"{task.validation_criteria}"

Does the output meet the criteria? Respond with a single JSON object: {{"is_valid": true}} or {{"is_valid": false}}.
"""
        # In a real system, you'd parse the LLM response. Here we mock a successful validation.
        logger.info(f"  🤖 (Simulated) Agent Prompt for validation:\n{prompt}")
        return True

    # --- Mock Tool Implementations ---
    # In a real system, these would call the actual tool logic.

    async def maestro_search_mock(self, **kwargs) -> dict:
        query = kwargs.get("query", "No query provided")
        return {"status": "success", "results": [{"title": f"Mock search result for '{query}'", "url": "https://example.com"}]}

    async def maestro_iae_mock(self, **kwargs) -> dict:
        input_data = kwargs.get("input", "No input provided")
        return {"status": "success", "analysis": f"Mock analysis of '{input_data}' shows positive trends."}

    async def maestro_execute_mock(self, **kwargs) -> dict:
        code = kwargs.get("code", "print('No code provided')")
        return {"status": "success", "stdout": f"Executed: {code}", "stderr": ""}

    async def maestro_error_handler_mock(self, **kwargs) -> dict:
        error = kwargs.get("error", "Unknown error")
        return {"status": "acknowledged", "recovery_suggestion": f"For error '{error}', try rebooting."} 